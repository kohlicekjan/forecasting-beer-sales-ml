{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting beer sales \n",
    "\n",
    "### Source\n",
    "- https://scikit-learn.org/stable/index.html\n",
    "- https://www.mariofilho.com/how-to-predict-multiple-time-series-with-scikit-learn-with-sales-forecasting-example/\n",
    "- https://alkaline-ml.com/pmdarima/quickstart.html\n",
    "- https://facebook.github.io/prophet/\n",
    "\n",
    "\n",
    "- https://catboost.ai/docs/concepts/python-reference_catboostregressor.html\n",
    "\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/\n",
    "- https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/\n"
   ]
  },
  {
   "source": [
    "## Install"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "\n",
    "# !pip install sklearn\n",
    "# !pip install joblib\n",
    "# !pip install optuna\n",
    "\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "# !pip install catboost\n",
    "\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install rfpimp\n",
    "# !pip install skater\n",
    "# !pip install eli5"
   ]
  },
  {
   "source": [
    "## Init"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting, enable_iterative_imputer\n",
    "from sklearn import datasets, linear_model, ensemble, gaussian_process, isotonic, kernel_ridge, neighbors, neural_network, svm, tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, max_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold,train_test_split\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, FunctionTransformer,  StandardScaler, LabelEncoder, LabelBinarizer, OrdinalEncoder, RobustScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "import lightgbm  \n",
    "import xgboost \n",
    "import catboost\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "import optuna\n",
    "\n",
    "# visualization lib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rfpimp import permutation_importances, plot_importances, plot_corr_heatmap\n",
    "# import shap\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "\n",
    "from helper import absolute_score, show_pred, show_feature_importances, reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'off-trade'\n",
    "\n",
    "DATA_PATH = f'./data/{DATASET_NAME}_data.csv'\n",
    "\n",
    "MODEL_PATH = f'./models/{DATASET_NAME}_model.joblib'\n",
    "SCALER_PATH = f'./models/{DATASET_NAME}_scaler.joblib'\n",
    "IMPUTER_PATH = f'./models/{DATASET_NAME}_imputer.joblib'\n",
    "SKU_ENCODER_PATH = f'./models/{DATASET_NAME}_sku_encoder.joblib'\n",
    "BRAND_ENCODER_PATH = f'./models/{DATASET_NAME}_brand_encoder.joblib'\n",
    "\n",
    "RESULT_CSV_PATH = f'./train_results/{DATASET_NAME}_train_result.csv'\n",
    "RESULT_EXCEL_PATH = f'./train_results/{DATASET_NAME}_train_result.xlsx'\n",
    "\n",
    "MAX_N_JOBS = 8"
   ]
  },
  {
   "source": [
    "## Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdays = pd.read_csv(f'./data/workdays.csv', header=0, decimal=\",\")\n",
    "\n",
    "workdays = workdays.rename(columns={\"Workdays\": \"NumberWorkdays\"})\n",
    "workdays.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtype = {'SkuShort':'str', 'Brand':'str', 'Country':'str', 'IsLockdown':'bool'} #'ProductGroup':'str', 'PrimaryPack':'str',\n",
    "\n",
    "data = pd.read_csv(DATA_PATH, header=0, decimal=\",\", dtype=data_dtype)\n",
    "\n",
    "# data_off_trade = pd.read_csv(f'./data/off-trade_data.csv', header=0, decimal=\",\", dtype=data_dtype)\n",
    "# data_on_trade = pd.read_csv(f'./data/on-trade_data.csv', header=0, decimal=\",\", dtype=data_dtype)\n",
    "# data = pd.concat([data_off_trade, data_on_trade], ignore_index=True)\n",
    "\n",
    "data = data.sort_values(by=['Year', 'Week', 'SkuShort', 'Country']).reset_index().drop(columns=['index'])\n",
    "\n",
    "if ('off-trade' in DATASET_NAME):\n",
    "    lastYear = data.Year.max()\n",
    "    data = data[(data.Year >= lastYear-2)]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reduce_mem_usage(data, ignoreCols=['SalesHl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "source": [
    "## Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "source": [
    "## Data Visualizations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=[12,12])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15,10))\n",
    "mask = np.zeros_like(data.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numData = data.select_dtypes(exclude='object')\n",
    "numDatacorr = numData.corr()\n",
    "salesHlFrame = numDatacorr['SalesHl'].sort_values(ascending=False).head(10).to_frame()\n",
    "\n",
    "salesHlFrame.style.background_gradient(cmap=sns.light_palette(\"cyan\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Year','Week']).sum().plot(y=['SalesHl'], figsize=(8,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Week']).mean().plot(y=['AvgTemp', 'AvgRain', 'AvgSun', 'SalesHl', 'PdtHl'], figsize=(8,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "g_pps = data.groupby('Brand')['SalesHl'].mean().sort_values(ascending=False)\n",
    "axis = sns.barplot(x=g_pps.index, y=g_pps, palette='autumn_r')\n",
    "axis.set_xlabel('Brand')\n",
    "axis.set_ylabel('SalesHl')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Skewness and Kurtesis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.kurt()"
   ]
  },
  {
   "source": [
    "## Spliting data in X and Y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['SalesHl', 'OldPredSalesHl'] #'SkuShort', 'ProductGroup', 'PrimaryPack', 'Country', 'Year', 'BgtHl', 'Brand', 'SubBrand'\n",
    "\n",
    "X = data.drop(cols_drop, axis=1)\n",
    "X_raw = X.copy(deep=True)\n",
    "y = data.SalesHl.fillna(0).to_numpy()\n",
    "y_oldPred = data.OldPredSalesHl.fillna(0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X['NextWeek'] = X['Week']+1\n",
    "# X = X.merge(workdays, how='left', left_on=['Country','Year','NextWeek'], right_on=['Country','Year','Week'], suffixes=(None, \"Next\"))\n",
    "# X = X.drop(['NextWeek',  'WeekNext'], axis=1) \n",
    "# X['NumberWorkdaysNext'] = X['NumberWorkdaysNext'].fillna(5)\n",
    "\n",
    "# X['PrevWeek'] = X['Week']-1\n",
    "# X = X.merge(workdays, how='left', left_on=['Country','Year','PrevWeek'], right_on=['Country','Year','Week'], suffixes=(None, \"Prev\"))\n",
    "# X = X.drop(['PrevWeek', 'WeekPrev', ], axis=1) #'NextWeek',  'WeekNext'\n",
    "# X['NumberWorkdaysPrev'] = X['NumberWorkdaysPrev'].fillna(5)\n",
    "\n",
    "X['NextWeek'] = X['Week']+1\n",
    "X['PrevWeek'] = X['Week']-1\n",
    "\n",
    "X = X.merge(workdays, how='left', left_on=['Country','Year','NextWeek'], right_on=['Country','Year','Week'], suffixes=(None, \"Next\"))\n",
    "X = X.merge(workdays, how='left', left_on=['Country','Year','PrevWeek'], right_on=['Country','Year','Week'], suffixes=(None, \"Prev\"))\n",
    "\n",
    "X = X.drop(['PrevWeek', 'WeekPrev', 'NextWeek',  'WeekNext'], axis=1)\n",
    "X['NumberWorkdaysPrev'] = X['NumberWorkdaysPrev'].fillna(5)\n",
    "X['NumberWorkdaysNext'] = X['NumberWorkdaysNext'].fillna(5)\n",
    "X"
   ]
  },
  {
   "source": [
    "## LabelEncoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Country_mapping = {\"CZ\": 1, \"SK\": 2}\n",
    "# PrimaryPack_mapping = {\"CAN\": 1, \"KEG\": 2, \"RB\": 3, \"NRB\": 4, \"TANK\": 5, \"PET\": 6, \"KEG ONE WAY\": 7, \"KEG WOODEN\": 8}\n",
    "# ProductGroup_mapping = {\"CLEAR BEER\": 1, \"FLAVOURED BEER\": 2, \"CIDER\": 3, \"SOFT DRINKS\": 4, \"MALT BASED BEV\": 5, \"WHEAT BEER\": 6, \"FABS - Seltzer sugar base\": 7, \"CLEAR BEER - MIXED PALLET (CZSK)\": 8}\n",
    "\n",
    "X.Country = X.Country.map(Country_mapping)\n",
    "# X.PrimaryPack = X.PrimaryPack.map(PrimaryPack_mapping)\n",
    "# X.ProductGroup =  X.ProductGroup.map(ProductGroup_mapping)\n",
    "\n",
    "sku_encoder = LabelEncoder()\n",
    "X.SkuShort = sku_encoder.fit_transform(X.SkuShort)\n",
    "\n",
    "brand_encoder = LabelEncoder()\n",
    "X.Brand = brand_encoder.fit_transform(X.Brand)\n",
    "# X.SubBrand = LabelEncoder().fit_transform(X.SubBrand)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "source": [
    "## Imputer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.BgtHl = X.BgtHl.fillna(0)\n",
    "X.PdtHl = X.PdtHl.fillna(0)\n",
    "X.PrevWeekPdtHl1 = X.PrevWeekPdtHl1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "# imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "# imputer = IterativeImputer(random_state=0, skip_complete=True, min_value=0) #649.619645\t\n",
    "# imputer = KNNImputer()\n",
    "\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "X"
   ]
  },
  {
   "source": [
    "## Adding a New Feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg \n",
    "X['AvgPrevWeekSalesHl'] = X[['PrevWeekSalesHl1', 'PrevWeekSalesHl2']].mean(axis=1).round(1)\n",
    "X['AvgPrevYearSalesHl'] = X[['PrevYearSalesHl1', 'PrevYearSalesHl2']].mean(axis=1).round(1)\n",
    "X['AvgPrevAllSalesHl'] = X[['PrevYearSalesHl1', 'PrevYearSalesHl2', 'PrevWeekSalesHl1', 'PrevWeekSalesHl2']].mean(axis=1).round(1)\n",
    "\n",
    "X"
   ]
  },
  {
   "source": [
    "## Split train and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = len(X[(X.Year <= 2020) | ((X.Year == 2021) & (X.Week < 15))])\n",
    "# train_index = len(X[(X.Year <= 2019) | ((X.Year == 2020) & (X.Week < 45))])\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 21)\n",
    " \n",
    "X_train = X.iloc[:train_index]\n",
    "y_train = y[:train_index]\n",
    "\n",
    "X_test = X.iloc[train_index:]\n",
    "X_test_raw = X_raw.iloc[train_index:]\n",
    "y_test = y[train_index:]\n",
    "y_test_oldPred = y_oldPred[train_index:]"
   ]
  },
  {
   "source": [
    "## Scaler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers = [\n",
    "#     (\"ignore1\", 'passthrough', ['SkuShort','Brand','Country','Year','Week','NumberWorkdays']),\n",
    "#     (\"num1\", StandardScaler(), ['AvgTemp', 'AvgRain', 'AvgSun']),\n",
    "#     (\"ignore2\", 'passthrough', ['IsLockdown']),\n",
    "#     (\"num2\", StandardScaler(), ['PdtHl', 'PrevWeekPdtHl1', 'BgtHl', 'PrevWeekSalesHl1', 'PrevWeekSalesHl2', 'PrevYearSalesHl1','PrevYearSalesHl2']),\n",
    "#     (\"ignore3\", 'passthrough', ['NumberWorkdaysNext', 'NumberWorkdaysPrev']),\n",
    "#     (\"num3\", StandardScaler(), ['AvgPrevWeekSalesHl','AvgPrevYearSalesHl','AvgPrevAllSalesHl']),\n",
    "# ]\n",
    "\n",
    "# transformer = ColumnTransformer(transformers = transformers)\n",
    "\n",
    "# X = pd.DataFrame(transformer.fit_transform(X), columns=X.columns)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# scaler = MaxAbsScaler()\n",
    "scaler = StandardScaler() \n",
    "# scaler = RobustScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "X_train"
   ]
  },
  {
   "source": [
    "## Regressors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = ensemble.AdaBoostRegressor()\n",
    "# model = tree.DecisionTreeRegressor(random_state=0)\n",
    "#model = ensemble.BaggingRegressor(base_estimator=svm.SVR(), bootstrap=True, bootstrap_features=False, n_estimators=200, oob_score=False, warm_start=True)\n",
    "#model = ensemble.ExtraTreesRegressor(n_jobs=3)\n",
    "# model = ensemble.GradientBoostingRegressor()\n",
    "#model = ensemble.RandomForestRegressor(n_estimators=1000, n_jobs=3, bootstrap=True, criterion='mse', oob_score=True)\n",
    "#NOT model = ensemble.StackingRegressor(estimators=[('lgbm', lgb.LGBMRegressor()),('hgb', ensemble.HistGradientBoostingRegressor())],final_estimator=ensemble.BaggingRegressor(), n_jobs=4, passthrough=False) \n",
    "#model = ensemble.VotingRegressor([('lgbm', lgb.LGBMRegressor(n_estimators=5000, learning_rate=0.005)), ('rf', ensemble.RandomForestRegressor(n_estimators=1000))])\n",
    "#model = ensemble.HistGradientBoostingRegressor(random_state=1, loss='least_squares', learning_rate=0.05, max_iter=350, max_leaf_nodes=70, early_stopping=False)\n",
    "\n",
    "#NOT model = gaussian_process.GaussianProcessRegressor()\n",
    "#NOT model = isotonic.IsotonicRegression()\n",
    "\n",
    "#NOT model = kernel_ridge.KernelRidge(alpha=1.0)\n",
    "\n",
    "#model = linear_model.LogisticRegression(max_iter=10)\n",
    "# model = linear_model.LinearRegression(copy_X=True, fit_intercept=True, normalize=False, positive=False)\n",
    "# model = linear_model.Ridge(max_iter=1000, copy_X=True, fit_intercept=True, normalize=True, solver='sparse_cg')\n",
    "# model = linear_model.SGDRegressor(max_iter=1500, early_stopping=False, alpha=0.0001, average=True, epsilon=0.1,eta0=0.01, fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling', loss='huber', penalty='l2', power_t=0.1, shuffle=True, warm_start=True)\n",
    "\n",
    "# model = linear_model.ElasticNet(max_iter=2000, alpha=0.001, copy_X=False, fit_intercept=True, l1_ratio=1, normalize=True, positive=False, precompute=False, selection='random', warm_start=True)\n",
    "# model = linear_model.Lars(n_nonzero_coefs=1000) \n",
    "# model = linear_model.LassoLars(max_iter=500) \n",
    "# model = linear_model.OrthogonalMatchingPursuit()\n",
    "# model = linear_model.ARDRegression(n_iter=500, compute_score=True, copy_X=True, fit_intercept=True, normalize=False) \n",
    "# model = linear_model.BayesianRidge(n_iter=500, compute_score=True, copy_X=True, fit_intercept=True, normalize=True) \n",
    "\n",
    "# model = linear_model.HuberRegressor(max_iter=500, epsilon=1.6, fit_intercept=True, warm_start=True) \n",
    "# model = linear_model.RANSACRegressor(max_trials=500)\n",
    "# model = linear_model.TheilSenRegressor(max_iter=500, n_jobs=-1)\n",
    "\n",
    "#NOT model = linear_model.PoissonRegressor(max_iter=500)\n",
    "# model = linear_model.TweedieRegressor(max_iter=500, alpha=0.05, fit_intercept=False, link='auto', power=0, warm_start=True) \n",
    "#NOT model = linear_model.GammaRegressor(max_iter=500) \n",
    "#NOT model = linear_model.PassiveAggressiveRegressor(random_state=0, fit_intercept=True) \n",
    "\n",
    "# model = neighbors.KNeighborsRegressor(n_neighbors=7, weights='uniform', leaf_size=30, n_jobs=-1) \n",
    "# model = neighbors.RadiusNeighborsRegressor(radius=5.0, weights='distance')\n",
    "\n",
    "#NOT model = svm.LinearSVR()\n",
    "\n",
    "#model = xgboost.XGBRegressor(random_state=0, tree_method='approx', booster='dart', colsample_bytree= 0.5, gamma=0.0, learning_rate=0.2, max_depth=3, reg_lambda=20, min_child_weight=2, n_jobs=MAX_N_JOBS)#0.81 #reg_lambda=20,"
   ]
  },
  {
   "source": [
    "## Find best params"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xgboost.XGBRegressor(random_state=0, tree_method='approx', booster='dart',  importance_type='gain', reg_lambda=10, reg_alpha=23, colsample_bytree= 0.5, gamma=0.0, learning_rate=0.2, max_depth=3, min_child_weight=2, n_jobs=MAX_N_JOBS)#0.81 #reg_lambda=20,\n",
    "\n",
    "# param_grid = dict(base_score =[ 0.25, 0.3, 0.35, 0.4,]) #importance_type=['gain', 'weight', 'cover','total_gain','total_cover']\n",
    "# clf = GridSearchCV(model, param_grid, n_jobs=2) #, random_state=0\n",
    "# search = clf.fit(X_train, y_train)\n",
    "# #print(search.cv_results_)\n",
    "# print(search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xgboost.XGBRegressor(random_state=0, tree_method='approx', booster='dart', importance_type='gain',  reg_lambda=10, reg_alpha=23, colsample_bytree= 0.5, gamma=0.0, learning_rate=0.2, max_depth=3, min_child_weight=2, n_estimators=100,n_jobs=MAX_N_JOBS)#0.9168\n",
    "\n",
    "# # Train the model using the training sets\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions using the testing set\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred[y_pred < 0] = 0\n",
    "# y_pred = y_pred.round(0)\n",
    "\n",
    "# print(DATASET_NAME)\n",
    "# print('Coefficient of determination: %.4f'% r2_score(y_test, y_pred))\n",
    "# print('Absolute Score: %.4f'% absolute_score(y_test, y_pred))\n",
    "# print('Mean absolute error regression loss.: %.4f'% mean_absolute_error(y_test, y_pred))\n",
    "# print('Maximum residual error: %.4f'% max_error(y_test, y_pred))\n",
    "# print('Mean squared error regression loss: %.4f'% mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "## Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### HistGradientBoostingRegressor "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbModel = ensemble.HistGradientBoostingRegressor(random_state=0, loss='least_squares', learning_rate=0.05, max_iter=250, max_leaf_nodes=150, min_samples_leaf=25, early_stopping=False)"
   ]
  },
  {
   "source": [
    "### MLPRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpModel = neural_network.MLPRegressor(random_state=0, activation='relu', solver='adam', hidden_layer_sizes=100, shuffle=False, warm_start=True, max_iter=1000, early_stopping=False)"
   ]
  },
  {
   "source": [
    "### RandomForestRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = ensemble.RandomForestRegressor(random_state=0, criterion='mse', min_samples_split=2, min_samples_leaf=3, n_estimators=100, max_depth=None, max_features=None, bootstrap=True, oob_score=True, n_jobs=MAX_N_JOBS)"
   ]
  },
  {
   "source": [
    "### XGBRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModel = xgboost.XGBRegressor(random_state=0, tree_method='approx', booster='dart', importance_type='gain',  reg_lambda=10, reg_alpha=23, colsample_bytree= 0.5, gamma=0.0, learning_rate=0.2, max_depth=3, min_child_weight=2, n_estimators=100,n_jobs=MAX_N_JOBS)"
   ]
  },
  {
   "source": [
    "### CatBoostRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbModel = catboost.CatBoostRegressor(learning_rate=0.05, eval_metric='RMSE', loss_function='RMSE', bootstrap_type='No', leaf_estimation_method='Newton', random_seed=42, verbose=0)"
   ]
  },
  {
   "source": [
    "### LGBMRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmModel = lightgbm.LGBMRegressor(random_state=0, boosting_type='goss', learning_rate=0.2, num_leaves=35, n_estimators=50, n_jobs=MAX_N_JOBS)"
   ]
  },
  {
   "source": [
    "### ExtraTreesRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etrModel = ensemble.ExtraTreesRegressor(bootstrap=True, criterion='mse', max_features='auto', oob_score=True, warm_start=True, n_estimators=200, min_weight_fraction_leaf=0, min_samples_split=2 , min_samples_leaf=2, ccp_alpha=0.7)"
   ]
  },
  {
   "source": [
    "### Use models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_models = [lgbmModel, cbModel, hgbModel, rfModel, etrModel] #xgbModel\n",
    "regression_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimators = []\n",
    "result_models = pd.DataFrame(columns=['Model', 'Score', \"AbsoluteScore\", 'ME', 'MSE', 'MAE'])\n",
    "\n",
    "for reg_model in regression_models:\n",
    "    \n",
    "    reg_model.fit(X_train, y_train)\n",
    "    y_pred = reg_model.predict(X_test)\n",
    "    \n",
    "    model_name = type(reg_model).__name__\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    abs_score = absolute_score(y_test, y_pred)\n",
    "    me = max_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    result_models.loc[len(result_models.index)] = [model_name, score, abs_score, me, mse, mae]\n",
    "\n",
    "    estimators.append((model_name, reg_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_metric = 'AbsoluteScore'\n",
    "max_score = result_models[col_metric].max()\n",
    "min_score = result_models[col_metric].min()\n",
    "min_weight = min_score - ((max_score - min_score) * 0.2)\n",
    "\n",
    "result_models[\"Weights\"] = result_models[col_metric].map(lambda x: round((x - min_weight) / (max_score - min_weight), 2))\n",
    "result_models.sort_values(by=[col_metric], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model in estimators:\n",
    "    y_pred = model.predict(X_test)\n",
    "    show_pred(y_test, y_pred, f\"{key} - Predicted Sales Hl\", \"True Sales Hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model in estimators:\n",
    "    show_feature_importances(model, X_test, y_test)"
   ]
  },
  {
   "source": [
    "### VotingRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = ensemble.VotingRegressor(estimators=estimators, weights=result_models.Weights.values, n_jobs=1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred = y_pred.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET_NAME)\n",
    "print('----------------')\n",
    "print('Coefficient of determination: %.4f'% r2_score(y_test, y_pred))\n",
    "print('Absolute Score: %.4f'% absolute_score(y_test, y_pred))\n",
    "print('Mean absolute error regression loss.: %.4f'% mean_absolute_error(y_test, y_pred))\n",
    "print('Maximum residual error: %.4f'% max_error(y_test, y_pred))\n",
    "print('Mean squared error regression loss: %.4f'% mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('----------------')\n",
    "\n",
    "print('Classic Coefficient of determination: %.4f'% r2_score(y_test, y_test_oldPred))\n",
    "print('Classic Absolute Score: %.4f'% absolute_score(y_test, y_test_oldPred))\n",
    "print('Classic Mean absolute error regression loss.: %.4f'% mean_absolute_error(y_test, y_test_oldPred))\n",
    "print('Classic Maximum residual error: %.4f'% max_error(y_test, y_test_oldPred))\n",
    "print('Classic Mean squared error regression loss: %.4f'% mean_squared_error(y_test, y_test_oldPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on-trade\n",
    "# ----------------\n",
    "# Coefficient of determination: 0.9582\n",
    "# Absolute Score: 0.8139\n",
    "# Mean absolute error regression loss.: 95.9693\n",
    "# Maximum residual error: 2890.5000\n",
    "# Mean squared error regression loss: 67592.0822\n",
    "\n",
    "\n",
    "# off-trade\n",
    "# ----------------\n",
    "# Coefficient of determination: 0.8583\n",
    "# Absolute Score: 0.6249\n",
    "# Mean absolute error regression loss.: 224.1787\n",
    "# Maximum residual error: 6374.9000\n",
    "# Mean squared error regression loss: 317901.6107\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred(y_test, y_pred, \"Predicted Sales Hl\", \"True Sales Hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "show_feature_importances(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = X_test_raw.copy(deep=True)\n",
    "result[\"TrueSalesHl\"] = y_test\n",
    "result[\"PredSalesHl_Classic\"] = y_test_oldPred\n",
    "result[\"PredSalesHl_ML\"] = y_pred\n",
    "\n",
    "dir_path = os.path.dirname(RESULT_CSV_PATH)\n",
    "if (not os.path.isdir(dir_path)):\n",
    "    os.mkdir(dir_path)\n",
    " \n",
    "result.to_csv(RESULT_CSV_PATH, index=False)\n",
    "result.to_excel(RESULT_EXCEL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(MODEL_PATH)\n",
    "if (not os.path.isdir(dir_path)):\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "joblib.dump(imputer, IMPUTER_PATH)\n",
    "joblib.dump(sku_encoder, SKU_ENCODER_PATH)\n",
    "joblib.dump(brand_encoder, BRAND_ENCODER_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}