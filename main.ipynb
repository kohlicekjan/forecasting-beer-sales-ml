{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting beer sales \n",
    "\n",
    "github: https://github.com/kohlicekjan/forecasting-beer-sales-ml\n",
    "\n",
    "### Source\n",
    "- https://scikit-learn.org/stable/index.html\n",
    "- https://www.mariofilho.com/how-to-predict-multiple-time-series-with-scikit-learn-with-sales-forecasting-example/\n",
    "- https://alkaline-ml.com/pmdarima/quickstart.html\n",
    "- https://facebook.github.io/prophet/\n",
    "\n",
    "\n",
    "- https://catboost.ai/docs/concepts/python-reference_catboostregressor.html"
   ]
  },
  {
   "source": [
    "## Init"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn import datasets, linear_model, ensemble, gaussian_process, isotonic, kernel_ridge, neighbors, neural_network, svm, tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, max_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, FunctionTransformer,  StandardScaler, LabelEncoder, LabelBinarizer, RobustScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "import lightgbm  \n",
    "import xgboost \n",
    "import catboost\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# visualization lib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from rfpimp import permutation_importances, plot_importances, plot_corr_heatmap\n",
    "\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# import shap\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_diff(previous, current):\n",
    "    return 1 - (abs(previous - current)/max(previous, current))\n",
    "\n",
    "def show_pred(y_test, y_pred, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_pred, y_test)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_feature_importances(model, X, y):\n",
    "    model_name = type(model).__name__\n",
    "    importances = None\n",
    "\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_       \n",
    "    else:\n",
    "        r = permutation_importance(model, X, y, n_repeats=5, random_state=42, n_jobs=MAX_N_JOBS)\n",
    "        importances = r.importances_std\n",
    "\n",
    "    sorted_idx = importances.argsort()\n",
    "\n",
    "    plt.figure(figsize=(8,6)) \n",
    "    plt.barh(X.columns[sorted_idx], importances[sorted_idx])\n",
    "    plt.xlabel(f\"{model_name} - Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:    \n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'on-trade'\n",
    "\n",
    "DATA_PATH = f'./data/{DATASET_NAME}_data.csv'\n",
    "\n",
    "MODEL_PATH = f'./models/{DATASET_NAME}_model.joblib'\n",
    "\n",
    "RESULT_CSV_PATH = f'./results/{DATASET_NAME}_result.csv'\n",
    "RESULT_EXCEL_PATH = f'./results/{DATASET_NAME}_result.xlsx'\n",
    "\n",
    "MAX_N_JOBS = 8"
   ]
  },
  {
   "source": [
    "## Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdays = pd.read_csv(f'./data/workdays.csv', header=0, decimal=\",\")\n",
    "\n",
    "workdays=workdays.rename(columns={\"Workdays\": \"NumberWorkdays\"})\n",
    "workdays.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtype = {'SkuShort':'str', 'ProductGroup':'str', 'PrimaryPack':'str', 'Country':'str', 'IsLockdown':'bool'}\n",
    "\n",
    "data = pd.read_csv(DATA_PATH, header=0, decimal=\",\", dtype=data_dtype)\n",
    "\n",
    "# data_off_trade = pd.read_csv(f'./data/off-trade_data.csv', header=0, decimal=\",\", dtype=data_dtype)\n",
    "# data_on_trade = pd.read_csv(f'./data/on-trade_data.csv', header=0, decimal=\",\", dtype=data_dtype)\n",
    "# data = pd.concat([data_off_trade, data_on_trade], ignore_index=True)\n",
    "\n",
    "data = data.sort_values(by=['Year','Week']).reset_index().drop(columns=['index'])\n",
    "\n",
    "if ('off-trade' in DATASET_NAME):\n",
    "    lastYear = data.Year.max()\n",
    "    data = data[(data.Year >= lastYear-3)]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reduce_mem_usage(data)\n",
    "data['SalesHl'] = data['SalesHl'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "source": [
    "## Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "source": [
    "## Data Visualizations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=[12,12])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplots(figsize = (15,10))\n",
    "mask = np.zeros_like(data.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Year','Week']).sum().plot(y=['SalesHl'], figsize=(8,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Week']).mean().plot(y=['AvgTemp', 'AvgRain', 'AvgSun', 'SalesHl', 'PdtHl'], figsize=(8,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "g_pps = data.groupby('PrimaryPack')['SalesHl'].mean().sort_values(ascending=False)\n",
    "axis = sns.barplot(x=g_pps.index, y=g_pps, palette='autumn_r')\n",
    "axis.set_xlabel('PrimaryPack')\n",
    "axis.set_ylabel('SalesHl')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "g_pgs = data.groupby('ProductGroup')['SalesHl'].mean().sort_values(ascending=False)\n",
    "axis = sns.barplot(x=g_pgs.index, y=g_pgs, palette='autumn_r')\n",
    "axis.set_xlabel('ProductGroup')\n",
    "axis.set_ylabel('SalesHl')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numData = data.select_dtypes(exclude='object')\n",
    "numDatacorr = numData.corr()\n",
    "salesHlFrame = numDatacorr['SalesHl'].sort_values(ascending=False).head(10).to_frame()\n",
    "\n",
    "salesHlFrame.style.background_gradient(cmap=sns.light_palette(\"cyan\", as_cmap=True))"
   ]
  },
  {
   "source": [
    "## Skewness and Kurtesis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.kurt()"
   ]
  },
  {
   "source": [
    "## Spliting data in X and Y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['SalesHl', 'OldPredSalesHl', 'ProductGroup', 'PrevWeekAvgTemp', 'PrevWeekAvgRain', 'PrevWeekAvgSun'] #'SkuShort', 'ProductGroup', 'PrimaryPack', 'Country', 'Year', 'PrevWeekAvgTemp', 'PrevWeekAvgRain', 'PrevWeekAvgSun' 'BgtHl',\n",
    "\n",
    "X = data.drop(cols_drop, axis=1)\n",
    "X_raw = data.drop(cols_drop, axis=1)\n",
    "y = pd.DataFrame(data.SalesHl).round(0).astype(int)\n",
    "y_oldPred = pd.DataFrame(data.OldPredSalesHl).fillna(0).round(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['NextWeek'] = X['Week']+1\n",
    "X['PrevWeek'] = X['Week']-1\n",
    "\n",
    "X = X.merge(workdays, how='left', left_on=['Country','Year','NextWeek'], right_on=['Country','Year','Week'], suffixes=(None, \"Next\"))\n",
    "X = X.merge(workdays, how='left', left_on=['Country','Year','PrevWeek'], right_on=['Country','Year','Week'], suffixes=(None, \"Prev\"))\n",
    "\n",
    "X = X.drop(['NextWeek', 'PrevWeek', 'WeekPrev', 'WeekNext'], axis=1)\n",
    "X['NumberWorkdaysPrev'] = X['NumberWorkdaysPrev'].fillna(5)\n",
    "X['NumberWorkdaysNext'] = X['NumberWorkdaysNext'].fillna(5)\n",
    "X"
   ]
  },
  {
   "source": [
    "## LabelEncoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Temp\n",
    "# bins = [-np.inf, -1, 1, 10, 15, 20, np.inf]\n",
    "# names = ['mraz', 'kolem nuly', 'chladno', 'teplo', 'velmi teplo', 'horko']\n",
    "# X['Temp'] = pd.cut(X['AvgTemp'], bins=bins, labels=names)\n",
    "# X['PrevWeekTemp'] = pd.cut(X['PrevWeekAvgTemp'], bins=bins, labels=names)\n",
    "\n",
    "# #create Rain\n",
    "# bins = [0, 0.1, 2.5, 8, 40, np.inf]\n",
    "# names = ['velmi slabá', 'slabá', 'mírná', 'silná', 'velmi silná']\n",
    "# X['Rain'] = pd.cut(X['AvgRain'], bins=bins, labels=names)\n",
    "# X['PrevWeekRain'] = pd.cut(X['PrevWeekAvgRain'], bins=bins, labels=names)\n",
    "\n",
    "# #create Sun\n",
    "# bins = [0, 2, 5, 8, np.inf]\n",
    "# names = ['zatazeno', 'oblacno', 'polojasno', 'jasno']\n",
    "# X['Sun'] = pd.cut(X['AvgSun'], bins=bins, labels=names)\n",
    "# X['PrevWeekSun'] = pd.cut(X['PrevWeekAvgSun'], bins=bins, labels=names)\n",
    "\n",
    "# X.Rain = LabelEncoder().fit_transform(X.Rain)\n",
    "# X.Temp = LabelEncoder().fit_transform(X.Temp)\n",
    "# X.Sun = LabelEncoder().fit_transform(X.Sun)\n",
    "# X.PrevWeekTemp = LabelEncoder().fit_transform(X.PrevWeekTemp)\n",
    "# X.PrevWeekRain = LabelEncoder().fit_transform(X.PrevWeekRain)\n",
    "# X.PrevWeekSun = LabelEncoder().fit_transform(X.PrevWeekSun)\n",
    "\n",
    "# X = X.drop(['AvgTemp', 'AvgRain', 'AvgSun', 'PrevWeekAvgTemp', 'PrevWeekAvgRain', 'PrevWeekAvgSun'], axis=1)\n",
    "\n",
    "#round weather\n",
    "# X.AvgTemp = X.AvgTemp.round(1)#.astype('int64')\n",
    "# X.AvgRain = X.AvgRain.round(1)#.astype('int64')\n",
    "# X.AvgSun = X.AvgSun.round(1)#.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Country_mapping = {\"CZ\": 1, \"SK\": 2}\n",
    "PrimaryPack_mapping = {\"CAN\": 1, \"KEG\": 2, \"RB\": 3, \"NRB\": 4, \"TANK\": 5, \"PET\": 6, \"KEG ONE WAY\": 7, \"KEG WOODEN\": 8}\n",
    "# ProductGroup_mapping = {\"CLEAR BEER\": 1, \"FLAVOURED BEER\": 2, \"CIDER\": 3, \"SOFT DRINKS\": 4, \"MALT BASED BEV\": 5, \"WHEAT BEER\": 6, \"FABS - Seltzer sugar base\": 7, \"CLEAR BEER - MIXED PALLET (CZSK)\": 8}\n",
    "\n",
    "X.Country = X.Country.map(Country_mapping)\n",
    "X.PrimaryPack = X.PrimaryPack.map(PrimaryPack_mapping)\n",
    "# X.ProductGroup =  X.ProductGroup.map(ProductGroup_mapping)\n",
    "\n",
    "el = LabelEncoder()\n",
    "X.SkuShort = el.fit_transform(X.SkuShort)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "source": [
    "## Imputer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.PdtHl = X.PdtHl.fillna(0)\n",
    "X.PrevWeekPdtHl1 = X.PrevWeekPdtHl1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SimpleImputer(strategy='mean')\n",
    "#si = SimpleImputer(strategy='constant', verbose=0)\n",
    "\n",
    "si.fit(X)\n",
    "# X_train = si.transform(X_train)\n",
    "# X_test = si.transform(X_test)\n",
    "X = pd.DataFrame(si.transform(X), columns=X.columns)"
   ]
  },
  {
   "source": [
    "## Adding a New Feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg \n",
    "X['AvgPrevWeekSalesHl'] = X[['PrevWeekSalesHl1', 'PrevWeekSalesHl2']].mean(axis=1).round(1)#.astype('int64')\n",
    "X['AvgPrevYearSalesHl'] = X[['PrevYearSalesHl1', 'PrevYearSalesHl2']].mean(axis=1).round(1)#.astype('int64')\n",
    "X['AvgPrevAllSalesHl'] = X[['PrevYearSalesHl1', 'PrevYearSalesHl2', 'PrevWeekSalesHl1', 'PrevWeekSalesHl2']].mean(axis=1).round(1)#.astype('int64')\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "source": [
    "## Split train and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = len(X[(X.Year <= 2020) | ((X.Year == 2021) & (X.Week < 10))])\n",
    "# train_index = len(X[(X.Year <= 2019) | ((X.Year == 2020) & (X.Week < 45))])\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 21)\n",
    "\n",
    "# X = X.drop(['Country', 'Year'], axis=1) #'PrimaryPack', 'Country', 'Year'\n",
    "X = X.drop(['Country', 'Year'], axis=1)\n",
    "\n",
    " \n",
    "X_train = X.iloc[:train_index]\n",
    "y_train = y.iloc[:train_index]\n",
    "\n",
    "X_test = X.iloc[train_index:]\n",
    "X_test_raw = X_raw.iloc[train_index:]\n",
    "y_test = y.iloc[train_index:]\n",
    "y_test_oldPred = y_oldPred.iloc[train_index:]"
   ]
  },
  {
   "source": [
    "## Scaler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# scaler = MaxAbsScaler()\n",
    "scaler = StandardScaler() \n",
    "# scaler = RobustScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "source": [
    "## Regressors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = ensemble.AdaBoostRegressor()\n",
    "# model = tree.DecisionTreeRegressor(random_state=0)\n",
    "#model = ensemble.BaggingRegressor(base_estimator=svm.SVR(), bootstrap=True, bootstrap_features=False, n_estimators=200, oob_score=False, warm_start=True)\n",
    "#model = ensemble.ExtraTreesRegressor(n_jobs=3)\n",
    "# model = ensemble.GradientBoostingRegressor()\n",
    "#model = ensemble.RandomForestRegressor(n_estimators=1000, n_jobs=3, bootstrap=True, criterion='mse', oob_score=True)\n",
    "#NOT model = ensemble.StackingRegressor(estimators=[('lgbm', lgb.LGBMRegressor()),('hgb', ensemble.HistGradientBoostingRegressor())],final_estimator=ensemble.BaggingRegressor(), n_jobs=4, passthrough=False) \n",
    "#model = ensemble.VotingRegressor([('lgbm', lgb.LGBMRegressor(n_estimators=5000, learning_rate=0.005)), ('rf', ensemble.RandomForestRegressor(n_estimators=1000))])\n",
    "#model = ensemble.HistGradientBoostingRegressor(random_state=1, loss='least_squares', learning_rate=0.05, max_iter=350, max_leaf_nodes=70, early_stopping=False)\n",
    "\n",
    "#NOT model = gaussian_process.GaussianProcessRegressor()\n",
    "#NOT model = isotonic.IsotonicRegression()\n",
    "\n",
    "#NOT model = kernel_ridge.KernelRidge(alpha=1.0)\n",
    "\n",
    "#model = linear_model.LogisticRegression(max_iter=10)\n",
    "# model = linear_model.LinearRegression(copy_X=True, fit_intercept=True, normalize=False, positive=False)\n",
    "# model = linear_model.Ridge(max_iter=1000, copy_X=True, fit_intercept=True, normalize=True, solver='sparse_cg')\n",
    "# model = linear_model.SGDRegressor(max_iter=1500, early_stopping=False, alpha=0.0001, average=True, epsilon=0.1,eta0=0.01, fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling', loss='huber', penalty='l2', power_t=0.1, shuffle=True, warm_start=True)\n",
    "\n",
    "# model = linear_model.ElasticNet(max_iter=2000, alpha=0.001, copy_X=False, fit_intercept=True, l1_ratio=1, normalize=True, positive=False, precompute=False, selection='random', warm_start=True)\n",
    "# model = linear_model.Lars(n_nonzero_coefs=1000) \n",
    "# model = linear_model.LassoLars(max_iter=500) \n",
    "# model = linear_model.OrthogonalMatchingPursuit()\n",
    "# model = linear_model.ARDRegression(n_iter=500, compute_score=True, copy_X=True, fit_intercept=True, normalize=False) \n",
    "# model = linear_model.BayesianRidge(n_iter=500, compute_score=True, copy_X=True, fit_intercept=True, normalize=True) \n",
    "\n",
    "# model = linear_model.HuberRegressor(max_iter=500, epsilon=1.6, fit_intercept=True, warm_start=True) \n",
    "# model = linear_model.RANSACRegressor(max_trials=500) #ON-TRADE: , OFF-TRADE: 0.7225\n",
    "# model = linear_model.TheilSenRegressor(max_iter=500, n_jobs=-1)\n",
    "\n",
    "#NOT model = linear_model.PoissonRegressor(max_iter=500)\n",
    "# model = linear_model.TweedieRegressor(max_iter=500, alpha=0.05, fit_intercept=False, link='auto', power=0, warm_start=True) \n",
    "#NOT model = linear_model.GammaRegressor(max_iter=500) \n",
    "#NOT model = linear_model.PassiveAggressiveRegressor(random_state=0, fit_intercept=True) \n",
    "\n",
    "# model = neighbors.KNeighborsRegressor(n_neighbors=7, weights='uniform', leaf_size=30, n_jobs=-1) \n",
    "# model = neighbors.RadiusNeighborsRegressor(radius=5.0, weights='distance')\n",
    "\n",
    "#NOT model = svm.LinearSVR()"
   ]
  },
  {
   "source": [
    "## Find best params"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = dict(learning_rate=[0.05], max_iter=[100], max_leaf_nodes=[140, 150, 160], min_samples_leaf=[25])\n",
    "# clf = GridSearchCV(model, param_grid, n_jobs=2) #, random_state=0\n",
    "# search = clf.fit(X_train, y_train.values.ravel())\n",
    "# #print(search.cv_results_)\n",
    "# print(search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model using the training sets\n",
    "# model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # Make predictions using the testing set\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred[y_pred < 0] = 0\n",
    "# y_pred = y_pred.round(0)\n",
    "\n",
    "# # # The mean squared error\n",
    "# # print('Mean squared error: %.4f'% mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# print(DATASET_NAME)\n",
    "# # The coefficient of determination: 1 is perfect prediction\n",
    "# print('Coefficient of determination: %.4f'% r2_score(y_test, y_pred))\n",
    "\n",
    "# # #Best possible score is 1.0, lower values are worse.\n",
    "# # print('Explained variance regression: %.4f'% explained_variance_score(y_test, y_pred))\n",
    "\n",
    "# #max_error metric calculates the maximum residual error.\n",
    "# print('Maximum residual error: %.4f'% max_error(y_test, y_pred))\n",
    "\n",
    "# # #Mean absolute percentage error regression loss.\n",
    "# print('Mean absolute percentage error regression loss: %.4f'% mean_absolute_percentage_error(y_test, y_pred))\n",
    "\n",
    "# print('Old Coefficient of determination: %.4f'% r2_score(y_test, y_test_oldPred))\n",
    "# print('Old Maximum residual error: %.4f'% max_error(y_test, y_test_oldPred))\n",
    "# print('Mean absolute percentage error regression loss: %.4f'% mean_absolute_percentage_error(y_test, y_test_oldPred))"
   ]
  },
  {
   "source": [
    "## Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### HistGradientBoostingRegressor "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbModel = ensemble.HistGradientBoostingRegressor(random_state=0, loss='least_squares', learning_rate=0.05, max_iter=250, max_leaf_nodes=150, min_samples_leaf=25, early_stopping=False)"
   ]
  },
  {
   "source": [
    "### MLPRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpModel = neural_network.MLPRegressor(random_state=0, activation='relu', solver='adam', hidden_layer_sizes=100, shuffle=False, warm_start=True, max_iter=1000, early_stopping=False)"
   ]
  },
  {
   "source": [
    "### RandomForestRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = ensemble.RandomForestRegressor(random_state=0, criterion='mse', min_samples_split=2, min_samples_leaf=3, n_estimators=100, max_depth=None, max_features=None, bootstrap=True, oob_score=True, n_jobs=MAX_N_JOBS)"
   ]
  },
  {
   "source": [
    "### XGBRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModel = xgboost.XGBRegressor(random_state=0, tree_method='approx', booster='dart', colsample_bytree= 0.5, gamma=0.0, learning_rate=0.2, max_depth=3, min_child_weight=2, n_jobs=MAX_N_JOBS)"
   ]
  },
  {
   "source": [
    "### CatBoostRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbModel = catboost.CatBoostRegressor(learning_rate=0.05, eval_metric='RMSE', loss_function='RMSE', bootstrap_type='No', leaf_estimation_method='Newton', random_seed=42, verbose=0)"
   ]
  },
  {
   "source": [
    "### LGBMRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmModel = lightgbm.LGBMRegressor(random_state=0, boosting_type='goss', learning_rate=0.2, num_leaves=35, n_estimators=50, n_jobs=MAX_N_JOBS)"
   ]
  },
  {
   "source": [
    "### ExtraTreesRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etrModel = ensemble.ExtraTreesRegressor(bootstrap=True, criterion='mse', max_features='auto', oob_score=True, warm_start=True, n_estimators=200, min_weight_fraction_leaf=0, min_samples_split=2 , min_samples_leaf=2, ccp_alpha=0.7)"
   ]
  },
  {
   "source": [
    "### Use models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_models = [lgbmModel, cbModel, hgbModel, rfModel,  etrModel] #xgbModel,\n",
    "regression_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimators = []\n",
    "result_models = pd.DataFrame(columns=['Model', 'Score', 'ME', 'MSE', 'MAE'])\n",
    "\n",
    "for reg_model in regression_models:\n",
    "    \n",
    "    reg_model.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = reg_model.predict(X_test)\n",
    "    \n",
    "    model_name = type(reg_model).__name__\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    me = max_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    result_models.loc[len(result_models.index)] = [model_name, score, me, mse, mae]\n",
    "\n",
    "    estimators.append((model_name, reg_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_models[\"Weights\"] = result_models['Score'].map(lambda x: round(x, 2))\n",
    "result_models.sort_values(by=['Score'], ascending=False)\n",
    "\n",
    "\n",
    "#off-trade\n",
    "# Model\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 3\tRandomForestRegressor\t0.866791\t333804.805779\t333804.805779\t244.611777\t0.87\n",
    "# 6\tLGBMRegressor\t0.864042\t340693.041703\t340693.041703\t254.249443\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.861159\t347916.449315\t347916.449315\t254.998749\t0.86\n",
    "# 0\tCatBoostRegressor\t0.849278\t377688.576685\t377688.576685\t261.573181\t0.85\n",
    "# 9\tGradientBoostingRegressor\t0.843769\t391493.638902\t391493.638902\t264.494670\t0.84\n",
    "# 4\tMLPRegressor\t0.839527\t402124.902408\t402124.902408\t278.039271\t0.84\n",
    "# 12\tLinearRegression\t0.832977\t418538.501574\t418538.501574\t308.129606\t0.83\n",
    "# 5\tLasso\t0.832871\t418803.072085\t418803.072085\t307.357371\t0.83\n",
    "# 7\tARDRegression\t0.832818\t418936.806781\t418936.806781\t307.672709\t0.83\n",
    "# 8\tBayesianRidge\t0.832793\t418999.803428\t418999.803428\t308.173667\t0.83\n",
    "# 11\tLinearSVR\t0.829922\t426193.915922\t426193.915922\t270.611320\t0.83\n",
    "# 10\tKNeighborsRegressor\t0.805067\t488476.376995\t488476.376995\t305.001409\t0.81\n",
    "# 1\tXGBRegressor\t0.772603\t569827.345052\t569827.345052\t297.754979\t0.77\n",
    "# 0.8639\n",
    "\n",
    "# Model\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.864042\t340693.041703\t340693.041703\t254.249443\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.861159\t347916.449315\t347916.449315\t254.998749\t0.86\n",
    "# 1\tCatBoostRegressor\t0.849278\t377688.576685\t377688.576685\t261.573181\t0.85\n",
    "# 3\tGradientBoostingRegressor\t0.844420\t389862.886930\t389862.886930\t264.334610\t0.84\n",
    "# 0.8668\n",
    "\n",
    "\n",
    "# \tModel\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 3\tRandomForestRegressor\t0.866791\t333804.805779\t333804.805779\t244.611777\t0.87\n",
    "# 0\tLGBMRegressor\t0.864042\t340693.041703\t340693.041703\t254.249443\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.861159\t347916.449315\t347916.449315\t254.998749\t0.86\n",
    "# 1\tCatBoostRegressor\t0.849278\t377688.576685\t377688.576685\t261.573181\t0.85\n",
    "# 4\tXGBRegressor\t0.772603\t569827.345052\t569827.345052\t297.754979\t0.77\n",
    "# 0.8658\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.877972\t4499.615238\t305751.570936\t243.381841\t0.88\n",
    "# 1\tCatBoostRegressor\t0.868797\t4848.307931\t328742.181746\t254.856241\t0.87\n",
    "# 3\tRandomForestRegressor\t0.866066\t4634.582000\t335582.718416\t245.259708\t0.87\n",
    "# 2\tHistGradientBoostingRegressor\t0.859194\t4445.449309\t352803.126808\t255.968944\t0.86\n",
    "# 4\tXGBRegressor\t0.846541\t5841.459961\t384504.761200\t266.124551\t0.85\n",
    "#0.8755\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.868797\t4848.307931\t328742.181746\t254.856241\t0.87\n",
    "# 2\tHistGradientBoostingRegressor\t0.864450\t4511.453271\t339633.416764\t250.393009\t0.86\n",
    "# 3\tRandomForestRegressor\t0.862192\t5083.518837\t345290.029548\t243.229869\t0.86\n",
    "# 0\tLGBMRegressor\t0.859358\t5338.457233\t352391.703972\t257.935003\t0.86\n",
    "# 4\tXGBRegressor\t0.846541\t5841.459961\t384504.761200\t266.124551\t0.85\n",
    "# 0.8735\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.876750\t4573.816964\t308813.474901\t242.787151\t0.88\n",
    "# 2\tHistGradientBoostingRegressor\t0.865286\t4480.964982\t337538.455249\t247.344246\t0.87\n",
    "# 3\tRandomForestRegressor\t0.863139\t4935.878258\t342917.150870\t242.075721\t0.86\n",
    "# 1\tCatBoostRegressor\t0.862816\t5980.891816\t343726.206302\t256.970952\t0.86\n",
    "# 5\tExtraTreesRegressor\t0.856400\t5185.426762\t359802.457086\t254.222853\t0.86\n",
    "# 4\tXGBRegressor\t0.848221\t5599.989258\t380296.917207\t264.753144\t0.85\n",
    "# 6\tHuberRegressor\t0.842212\t5884.212939\t395352.843826\t269.894448\t0.84\n",
    "#0.8778\n",
    "\n",
    "\n",
    "# \tModel\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.876750\t4573.816964\t308813.474901\t242.787151\t0.88\n",
    "# 2\tHistGradientBoostingRegressor\t0.865286\t4480.964982\t337538.455249\t247.344246\t0.87\n",
    "# 3\tRandomForestRegressor\t0.863139\t4935.878258\t342917.150870\t242.075721\t0.86\n",
    "# 1\tCatBoostRegressor\t0.862816\t5980.891816\t343726.206302\t256.970952\t0.86\n",
    "# 4\tExtraTreesRegressor\t0.854816\t4920.532056\t363770.800687\t256.005848\t0.85\n",
    "# 0.8760\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.881453\t4476.110411\t297031.174702\t247.456136\t0.88\n",
    "# 2\tHistGradientBoostingRegressor\t0.870817\t4444.321330\t323679.379881\t247.669848\t0.87\n",
    "# 1\tCatBoostRegressor\t0.865533\t5666.760007\t336919.717836\t257.028054\t0.87\n",
    "# 3\tRandomForestRegressor\t0.863930\t4936.985496\t340935.891273\t242.156392\t0.86\n",
    "# 5\tExtraTreesRegressor\t0.859392\t4718.446746\t352306.400799\t256.133988\t0.86\n",
    "# 4\tXGBRegressor\t0.843604\t5721.456055\t391864.071047\t268.586562\t0.84\n",
    "# 0.8766, 4140.0000\n",
    "\n",
    "\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.880940\t4479.946040\t298315.021833\t242.243320\t0.88\n",
    "# 2\tHistGradientBoostingRegressor\t0.871560\t4155.136411\t321816.920083\t240.828814\t0.87\n",
    "# 3\tRandomForestRegressor\t0.863485\t5242.699313\t342051.954132\t240.268927\t0.86\n",
    "# 1\tCatBoostRegressor\t0.860366\t5857.075787\t349865.577737\t251.530287\t0.86\n",
    "# 4\tExtraTreesRegressor\t0.858865\t4948.830381\t353625.965991\t253.925788\t0.86\n",
    "# 0.8772, 4113.0000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#on-trade\n",
    "# Model\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 6\tLGBMRegressor\t0.891683\t46193.121041\t46193.121041\t88.423377\t0.89\n",
    "# 1\tXGBRegressor\t0.890533\t46683.289154\t46683.289154\t97.600362\t0.89\n",
    "# 0\tCatBoostRegressor\t0.859515\t59911.323973\t59911.323973\t98.200393\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.859226\t60034.513630\t60034.513630\t91.230857\t0.86\n",
    "# 9\tGradientBoostingRegressor\t0.842605\t67122.924254\t67122.924254\t106.539311\t0.84\n",
    "# 10\tKNeighborsRegressor\t0.815448\t78704.180046\t78704.180046\t135.666990\t0.82\n",
    "# 3\tRandomForestRegressor\t0.813403\t79576.440824\t79576.440824\t94.941833\t0.81\n",
    "# 12\tLinearRegression\t0.811924\t80207.259648\t80207.259648\t170.711040\t0.81\n",
    "# 5\tLasso\t0.810957\t80619.289766\t80619.289766\t169.174454\t0.81\n",
    "# 7\tARDRegression\t0.810064\t81000.421378\t81000.421378\t170.797118\t0.81\n",
    "# 8\tBayesianRidge\t0.808762\t81555.376370\t81555.376370\t171.263265\t0.81\n",
    "# 11\tLinearSVR\t0.791987\t88709.547162\t88709.547162\t123.906669\t0.79\n",
    "# 4\tMLPRegressor\t0.599296\t170884.695195\t170884.695195\t177.887054\t0.60\n",
    "# 0.8598\n",
    "\n",
    "\n",
    "# Model\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.891683\t46193.121041\t46193.121041\t88.423377\t0.89\n",
    "# 1\tCatBoostRegressor\t0.859515\t59911.323973\t59911.323973\t98.200393\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.859226\t60034.513630\t60034.513630\t91.230857\t0.86\n",
    "# 3\tGradientBoostingRegressor\t0.834394\t70624.630360\t70624.630360\t107.111241\t0.83\n",
    "# 0.8747\n",
    "\n",
    "\n",
    "# Model\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.891683\t46193.121041\t46193.121041\t88.423377\t0.89\n",
    "# 1\tCatBoostRegressor\t0.859515\t59911.323973\t59911.323973\t98.200393\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.859226\t60034.513630\t60034.513630\t91.230857\t0.86\n",
    "# 3\tRandomForestRegressor\t0.813403\t79576.440824\t79576.440824\t94.941833\t0.81\n",
    "# 0.8717\n",
    "\n",
    "\n",
    "# \tModel\tScore\tMSE\tRMSE\tMAE\tWeights\n",
    "# 0\tLGBMRegressor\t0.891683\t46193.121041\t46193.121041\t88.423377\t0.89\n",
    "# 4\tXGBRegressor\t0.890533\t46683.289154\t46683.289154\t97.600362\t0.89\n",
    "# 1\tCatBoostRegressor\t0.859515\t59911.323973\t59911.323973\t98.200393\t0.86\n",
    "# 2\tHistGradientBoostingRegressor\t0.859226\t60034.513630\t60034.513630\t91.230857\t0.86\n",
    "# 3\tRandomForestRegressor\t0.813403\t79576.440824\t79576.440824\t94.941833\t0.81\n",
    "#0.8798\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.886093\t1951.314928\t48567.852765\t97.326672\t0.89\n",
    "# 2\tHistGradientBoostingRegressor\t0.869292\t2951.192139\t55731.458495\t89.914484\t0.87\n",
    "# 0\tLGBMRegressor\t0.838248\t3581.780052\t68968.260265\t98.742636\t0.84\n",
    "# 4\tXGBRegressor\t0.820312\t2580.676025\t76615.703155\t115.247313\t0.82\n",
    "# 3\tRandomForestRegressor\n",
    "# 0.860\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.886093\t1951.314928\t48567.852765\t97.326672\t0.89\n",
    "# 2\tHistGradientBoostingRegressor\t0.878109\t2105.323827\t51972.169158\t95.063952\t0.88\n",
    "# 0\tLGBMRegressor\t0.875615\t3169.248731\t53035.397007\t93.709183\t0.88\n",
    "# 3\tRandomForestRegressor\t0.842970\t3282.521439\t66954.547602\t93.920232\t0.84\n",
    "# 4\tXGBRegressor\t0.820312\t2580.676025\t76615.703155\t115.247313\t0.82\n",
    "# 0.8798\n",
    "\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.883171\t2157.891877\t49813.626724\t104.215533\t0.88\n",
    "# 0\tLGBMRegressor\t0.878800\t3029.242877\t51677.636250\t94.861470\t0.88\n",
    "# 2\tHistGradientBoostingRegressor\t0.872957\t2470.649197\t54168.848480\t95.944487\t0.87\n",
    "# 3\tRandomForestRegressor\t0.844283\t3215.982887\t66394.954947\t93.763728\t0.84\n",
    "# 5\tExtraTreesRegressor\t0.827780\t3833.764742\t73431.584507\t94.676410\t0.83\n",
    "# 4\tXGBRegressor\t0.812619\t2468.038574\t79895.776510\t114.346503\t0.81\n",
    "# 6\tHuberRegressor\t0.798678\t2931.472186\t85840.170854\t120.555470\t0.80\n",
    "# 0.8675\n",
    "\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.883171\t2157.891877\t49813.626724\t104.215533\t0.88\n",
    "# 0\tLGBMRegressor\t0.878800\t3029.242877\t51677.636250\t94.861470\t0.88\n",
    "# 2\tHistGradientBoostingRegressor\t0.872957\t2470.649197\t54168.848480\t95.944487\t0.87\n",
    "# 3\tRandomForestRegressor\t0.844283\t3215.982887\t66394.954947\t93.763728\t0.84\n",
    "# 4\tExtraTreesRegressor\t0.837165\t3755.308623\t69429.831443\t92.372810\t0.84\n",
    "# 0.8731, 3007.0000\n",
    "\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.897287\t1913.845427\t43794.754659\t94.319321\t0.90\n",
    "# 0\tLGBMRegressor\t0.887499\t3183.665807\t47968.436237\t86.968840\t0.89\n",
    "# 2\tHistGradientBoostingRegressor\t0.882464\t2419.912469\t50115.002449\t93.875961\t0.88\n",
    "# 3\tRandomForestRegressor\t0.844800\t3261.584169\t66174.599554\t93.106406\t0.84\n",
    "# 4\tXGBRegressor\t0.834410\t3122.962891\t70604.492375\t111.953248\t0.83\n",
    "# 5\tExtraTreesRegressor\t0.824119\t4149.834659\t74992.264525\t92.845915\t0.82\n",
    "# 0.8755, 2938.0000\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.906500\t1620.536292\t39866.571585\t91.589080\t0.91\n",
    "# 2\tHistGradientBoostingRegressor\t0.880496\t2233.264712\t50954.193725\t94.697780\t0.88\n",
    "# 0\tLGBMRegressor\t0.861194\t3443.464711\t59184.455358\t95.829477\t0.86\n",
    "# 4\tXGBRegressor\t0.846052\t3321.312500\t65640.444875\t109.056269\t0.85\n",
    "# 3\tRandomForestRegressor\t0.844228\t3254.442526\t66418.462063\t93.688319\t0.84\n",
    "# 5\tExtraTreesRegressor\t0.826406\t3869.360925\t74017.349860\t94.686626\t0.83\n",
    "# 0.8735, 2963.0000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.886093\t1951.314928\t48567.852765\t97.326672\t0.89\n",
    "# 2\tHistGradientBoostingRegressor\t0.878109\t2105.323827\t51972.169158\t95.063952\t0.88\n",
    "# 0\tLGBMRegressor\t0.867987\t3060.219495\t56288.079762\t93.129330\t0.87\n",
    "# 3\tRandomForestRegressor\t0.842970\t3282.521439\t66954.547602\t93.920232\t0.84\n",
    "# 5\tExtraTreesRegressor\t0.832147\t3796.008042\t71569.344861\t92.839672\t0.83\n",
    "# 4\tXGBRegressor\t0.820312\t2580.676025\t76615.703155\t115.247313\t0.82\n",
    "# 0.8715, 2756.0000\n",
    "\n",
    "# Model\tScore\tME\tMSE\tMAE\tWeights\n",
    "# 1\tCatBoostRegressor\t0.905673\t1519.024536\t40219.287135\t95.758263\t0.91\n",
    "# 0\tLGBMRegressor\t0.889466\t2490.981719\t47129.533793\t85.683087\t0.89\n",
    "# 2\tHistGradientBoostingRegressor\t0.866946\t2271.122606\t56731.784934\t96.094545\t0.87\n",
    "# 3\tRandomForestRegressor\t0.846924\t3151.606606\t65268.863710\t93.424762\t0.85\n",
    "# 4\tExtraTreesRegressor\t0.825878\t3996.722994\t74242.565505\t93.521422\t0.83\n",
    "# 0.8835, 2663.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model in estimators:\n",
    "    y_pred = model.predict(X_test)\n",
    "    show_pred(y_test, y_pred, f\"{key} - Predicted Sales Hl\", \"True Sales Hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model in estimators:\n",
    "    show_feature_importances(model, X_test, y_test)"
   ]
  },
  {
   "source": [
    "### VotingRegressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = ensemble.VotingRegressor(estimators=estimators, weights=result_models.Weights.values, n_jobs=1)\n",
    "\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred = y_pred.round(0)\n",
    "\n",
    "# # The mean squared error\n",
    "# print('Mean squared error: %.4f'% mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(DATASET_NAME)\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.4f'% r2_score(y_test, y_pred))\n",
    "\n",
    "# #Best possible score is 1.0, lower values are worse.\n",
    "# print('Explained variance regression: %.4f'% explained_variance_score(y_test, y_pred))\n",
    "\n",
    "#max_error metric calculates the maximum residual error.\n",
    "print('Maximum residual error: %.4f'% max_error(y_test, y_pred))\n",
    "\n",
    "# #Mean absolute percentage error regression loss.\n",
    "print('Mean absolute percentage error regression loss: %.4f'% mean_absolute_percentage_error(y_test, y_pred))\n",
    "\n",
    "print('Old Coefficient of determination: %.4f'% r2_score(y_test, y_test_oldPred))\n",
    "print('Old Maximum residual error: %.4f'% max_error(y_test, y_test_oldPred))\n",
    "print('Mean absolute percentage error regression loss: %.4f'% mean_absolute_percentage_error(y_test, y_test_oldPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred(y_test, y_pred, \"Predicted Sales Hl\", \"True Sales Hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "show_feature_importances(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = X_test_raw.copy(deep=False)\n",
    "result[\"SalesHl\"] = y_test\n",
    "result[\"OldPredSalesHl\"] = y_test_oldPred\n",
    "result[\"PredictSalesHl\"] = y_pred\n",
    "\n",
    "resultWeek = result.groupby(['Year','Week']).sum()\n",
    "resultWeek = resultWeek[['SalesHl', 'OldPredSalesHl', 'PredictSalesHl']].apply(lambda x : pd.Series({'OldPredSalesHl': get_percentage_diff(x[0], x[1]), 'NewPredSalesHl': get_percentage_diff(x[0], x[2])}) , axis=1)\n",
    "resultWeek['NewPredSalesHlDiff'] = resultWeek.NewPredSalesHl-resultWeek.OldPredSalesHl\n",
    "resultWeek = (100. * resultWeek).round(1).astype(str) + '%'\n",
    "print(resultWeek)\n",
    "\n",
    "resultYear = result.groupby(['Year']).sum()\n",
    "resultYear= resultYear[['SalesHl', 'OldPredSalesHl', 'PredictSalesHl']].apply(lambda x : pd.Series({'OldPredSalesHl': get_percentage_diff(x[0], x[1]), 'NewPredSalesHl': get_percentage_diff(x[0], x[2])}) , axis=1)\n",
    "resultYear['NewPredSalesHlDiff'] = resultYear.NewPredSalesHl-resultYear.OldPredSalesHl\n",
    "resultYear = (100. * resultYear).round(1).astype(str) + '%'\n",
    "print(resultYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labels =  X.columns\n",
    "\n",
    "# examples_numpy = pd.concat([pd.DataFrame(X_train).sample(n=0), pd.DataFrame(X_test).sample(n=700)]).to_numpy()\n",
    "examples_numpy = X_test\n",
    "\n",
    "# wrap our base model with InMemoryModel instance\n",
    "annotated_model = InMemoryModel(\n",
    "    model.predict, \n",
    "    examples = examples_numpy, \n",
    "    model_type = 'regressor'\n",
    ")\n",
    "\n",
    "interpreter = Interpretation(examples_numpy, feature_names=X_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name ,model = estimators[0]\n",
    "\n",
    "\n",
    "# examples_numpy = pd.concat([pd.DataFrame(X_train).sample(n=0), pd.DataFrame(X_test).sample(n=700)]).to_numpy()\n",
    "examples_numpy = X_test.to_numpy()\n",
    "\n",
    "# wrap our base model with InMemoryModel instance\n",
    "annotated_model = InMemoryModel(\n",
    "    model.predict, \n",
    "    examples = examples_numpy, \n",
    "    model_type = 'regressor'\n",
    ")\n",
    "\n",
    "interpreter = Interpretation(examples_numpy, feature_names=X_labels)\n",
    "\n",
    "interpreter.feature_importance.plot_feature_importance(annotated_model, progressbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importances(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=0, n_jobs=MAX_N_JOBS)\n",
    "importances = r.importances_std\n",
    "\n",
    "sorted_idx = importances.argsort()\n",
    "\n",
    "plt.figure(figsize=(8,6)) \n",
    "plt.barh(X_labels[sorted_idx], importances[sorted_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "perm = PermutationImportance(model, cv = None, refit = False, n_iter = 50).fit(X_test, y_test)\n",
    "sorted_idx = perm.feature_importances_.argsort()\n",
    "\n",
    "plt.figure(figsize=(8,6)) \n",
    "plt.barh(X_labels[sorted_idx], perm.feature_importances_[sorted_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter.partial_dependence.plot_partial_dependence(\n",
    "#     feature_names, annotated_model, grid_resolution=20, progressbar=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # create an explainer\n",
    "# explainer = LimeTabularExplainer(examples_numpy, feature_names=feature_names, mode=\"regression\")\n",
    "\n",
    "# # explain something\n",
    "# explanation = explainer.explain_instance(examples_numpy[5], annotated_model)\n",
    "\n",
    "# # show the explanation\n",
    "# explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "X100 = shap.utils.sample(X, 100)\n",
    "\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X100)\n",
    "\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.force(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.scatter(shap_values, color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = X_test_raw.copy(deep=True)\n",
    "result[\"SalesHl\"] = y_test\n",
    "result[\"OldPredSalesHl\"] = y_test_oldPred\n",
    "result[\"PredictSalesHl\"] = y_pred\n",
    "\n",
    "dir_path = os.path.dirname(RESULT_CSV_PATH)\n",
    "if (not os.path.isdir(dir_path)):\n",
    "    os.mkdir(dir_path)\n",
    " \n",
    "result.to_csv(RESULT_CSV_PATH, index=False)\n",
    "result.to_excel(RESULT_EXCEL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(MODEL_PATH)\n",
    "if (not os.path.isdir(dir_path)):\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "joblib.dump(model, MODEL_PATH, compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file = open(MODEL_PATH, 'rb')\n",
    "\n",
    "# model = joblib.load(file)\n",
    "\n",
    "# SkuShort = 2605\n",
    "# Week = 23\n",
    "# NumberWorkdays = 5\n",
    "# AvgTemp = 15.892857142857142\n",
    "# AvgRain = 3.5000000000000004\n",
    "# AvgSun = 6.735714285714286\n",
    "# IsLockdown = 0\n",
    "# PdtHl = -1.0\n",
    "# PrevWeekPdtHl1 = -1.0\n",
    "# BgtHl = 6665\n",
    "# PrevWeekBgtHl1 = 6665.949490847161\n",
    "# PrevWeekSalesHl1 = 5020\n",
    "# PrevWeekSalesHl2 = 5038\n",
    "# SalesHl = 5386.5\n",
    "\n",
    "# x = np.array([[SkuShort, Week,NumberWorkdays, AvgTemp, AvgRain, AvgSun, IsLockdown, PdtHl, PrevWeekPdtHl1, BgtHl, PrevWeekBgtHl1, PrevWeekSalesHl1, PrevWeekSalesHl2]])\n",
    "\n",
    "# y_pred = model.predict(x)\n",
    "# result = y_pred[0]\n",
    "\n",
    "# def get_percentage_diff(previous, current):\n",
    "#     return 1 - (abs(previous - current)/max(previous, current))\n",
    "\n",
    "# print('Forecast sales: %.4f hl'% result)\n",
    "# print('Coefficient of determination: %.4f'% get_percentage_diff(SalesHl, result))\n",
    "\n",
    "#full with sku = 0.7678\n",
    "#full = 0.7363"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}